{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: Milestone I Natural Language Processing\n",
    "## Task 1. Basic Text Pre-processing\n",
    "#### Student Name: Gayathri Devi Thotappa\n",
    "#### Student ID: s4111690\n",
    "\n",
    "\n",
    "Environment: Python 3 and Jupyter notebook\n",
    "\n",
    "Libraries used:  \n",
    "* pandas\n",
    "* re\n",
    "* numpy\n",
    "* nltk\n",
    "\n",
    "\n",
    "## Introduction\n",
    "This task prepares the raw clothing‐review data for downstream modelling by converting unstructured text into a clean, consistent, and reproducible representation. We work with ~19.6k reviews and focus only on the “Review Text” field (the title is ignored in Task 1). The goal is to remove noise, standardise tokens, remove stopwords, Prunes rare and overly common terms and build a vocabulary that accurately reflects meaningful language use in the corpus so that Task 2 (feature construction) and Task 3 (classification) are built on a solid foundation.\n",
    "\n",
    "Outputs:\n",
    "\n",
    "* processed.csv — cleaned tokens per review, ready for vectorisation \n",
    "\n",
    "* vocab.txt — an alphabetically sorted unigram vocabulary in the format word:index starting from 0, which is the key for interpreting sparse encodings in later tasks.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to import libraries as you need in this assessment \n",
    "import pandas as pd \n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Examining and loading data\n",
    "We load the provided dataset from assignment3.csv, which contains the Title, Review Text, and the target label Recommended IND (0 = not recommended, 1 = recommended). To ensure the text is ready for processing, the Review Text field is coerced to string and missing values are filled with empty strings. As a quick sanity check, we confirm the total number of rows, look for missing values in critical columns, examine basic length statistics of the review text, and inspect the class balance of the target label. These checks help surface common issues (for example, nulls, empty reviews, severe class imbalance) before we proceed to tokenisation and vocabulary construction in Task 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews: 19662\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Clothing ID</th>\n",
       "      <th>Age</th>\n",
       "      <th>Title</th>\n",
       "      <th>Review Text</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Recommended IND</th>\n",
       "      <th>Positive Feedback Count</th>\n",
       "      <th>Division Name</th>\n",
       "      <th>Department Name</th>\n",
       "      <th>Class Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1077</td>\n",
       "      <td>60</td>\n",
       "      <td>Some major design flaws</td>\n",
       "      <td>I had such high hopes for this dress and reall...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>General</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>Dresses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1049</td>\n",
       "      <td>50</td>\n",
       "      <td>My favorite buy!</td>\n",
       "      <td>I love, love, love this jumpsuit. it's fun, fl...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>General Petite</td>\n",
       "      <td>Bottoms</td>\n",
       "      <td>Pants</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>847</td>\n",
       "      <td>47</td>\n",
       "      <td>Flattering shirt</td>\n",
       "      <td>This shirt is very flattering to all due to th...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>General</td>\n",
       "      <td>Tops</td>\n",
       "      <td>Blouses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1080</td>\n",
       "      <td>49</td>\n",
       "      <td>Not for the very petite</td>\n",
       "      <td>I love tracy reese dresses, but this one is no...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>General</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>Dresses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>858</td>\n",
       "      <td>39</td>\n",
       "      <td>Cagrcoal shimmer fun</td>\n",
       "      <td>I aded this in my basket at hte last mintue to...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>General Petite</td>\n",
       "      <td>Tops</td>\n",
       "      <td>Knits</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Clothing ID  Age                    Title  \\\n",
       "0         1077   60  Some major design flaws   \n",
       "1         1049   50         My favorite buy!   \n",
       "2          847   47         Flattering shirt   \n",
       "3         1080   49  Not for the very petite   \n",
       "4          858   39     Cagrcoal shimmer fun   \n",
       "\n",
       "                                         Review Text  Rating  Recommended IND  \\\n",
       "0  I had such high hopes for this dress and reall...       3                0   \n",
       "1  I love, love, love this jumpsuit. it's fun, fl...       5                1   \n",
       "2  This shirt is very flattering to all due to th...       5                1   \n",
       "3  I love tracy reese dresses, but this one is no...       2                0   \n",
       "4  I aded this in my basket at hte last mintue to...       5                1   \n",
       "\n",
       "   Positive Feedback Count   Division Name Department Name Class Name  \n",
       "0                        0         General         Dresses    Dresses  \n",
       "1                        0  General Petite         Bottoms      Pants  \n",
       "2                        6         General            Tops    Blouses  \n",
       "3                        4         General         Dresses    Dresses  \n",
       "4                        1  General Petite            Tops      Knits  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Code to inspect the provided data file \n",
    "# Load raw data and make sure critical columns exist; coerce Review Text to string\n",
    "data_csv_path = \"assignment3.csv\"    \n",
    "df = pd.read_csv(data_csv_path)\n",
    "\n",
    "# Ensure Review Text is string\n",
    "df[\"Review Text\"] = df[\"Review Text\"].fillna(\"\").astype(str)\n",
    "\n",
    "#display the length of reviews \n",
    "print(\"Number of reviews:\", len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Pre-processing data\n",
    "In text preprocessing, convert raw Review Text into a clean, consistent form suitable for modelling. Text is tokenised with the required regex ([a-zA-Z]+(?:[-'][a-zA-Z]+)?) to keep valid words (including hyphen/apostrophe forms), then lower-cased and very short tokens are removed to reduce noise. We remove stopwords (from stopwords_en.txt) so common function words don’t dominate. Next, we prune rare terms that appear only once across the corpus and drop the top-20 highest document-frequency words to improve discrimination. The result is a compact, informative vocabulary and cleaned tokens saved to processed.csv and vocab.txt, providing a reproducible foundation for Task 2 features and Task 3 classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review Text</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I had such high hopes for this dress and reall...</td>\n",
       "      <td>[I, had, such, high, hopes, for, this, dress, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I love, love, love this jumpsuit. it's fun, fl...</td>\n",
       "      <td>[I, love, love, love, this, jumpsuit, it's, fu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This shirt is very flattering to all due to th...</td>\n",
       "      <td>[This, shirt, is, very, flattering, to, all, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I love tracy reese dresses, but this one is no...</td>\n",
       "      <td>[I, love, tracy, reese, dresses, but, this, on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I aded this in my basket at hte last mintue to...</td>\n",
       "      <td>[I, aded, this, in, my, basket, at, hte, last,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Review Text  \\\n",
       "0  I had such high hopes for this dress and reall...   \n",
       "1  I love, love, love this jumpsuit. it's fun, fl...   \n",
       "2  This shirt is very flattering to all due to th...   \n",
       "3  I love tracy reese dresses, but this one is no...   \n",
       "4  I aded this in my basket at hte last mintue to...   \n",
       "\n",
       "                                              tokens  \n",
       "0  [I, had, such, high, hopes, for, this, dress, ...  \n",
       "1  [I, love, love, love, this, jumpsuit, it's, fu...  \n",
       "2  [This, shirt, is, very, flattering, to, all, d...  \n",
       "3  [I, love, tracy, reese, dresses, but, this, on...  \n",
       "4  [I, aded, this, in, my, basket, at, hte, last,...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# code to perform the task...\n",
    "# Tokenization — keep alphabetic tokens and common hyphen/apostrophe forms as per spec\n",
    "tokenizer = RegexpTokenizer(r\"[a-zA-Z]+(?:[-'][a-zA-Z]+)?\")\n",
    "\n",
    "def tokenize(text):\n",
    "    return tokenizer.tokenize(text)\n",
    "\n",
    "# Apply to dataset\n",
    "df[\"tokens\"] = df[\"Review Text\"].apply(tokenize)\n",
    "df[[\"Review Text\", \"tokens\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopwords loaded: 570\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    [high, hopes, dress, wanted, work, initially, ...\n",
       "1    [love, love, love, jumpsuit, fun, flirty, fabu...\n",
       "2    [shirt, flattering, due, adjustable, front, ti...\n",
       "3    [love, tracy, reese, dresses, petite, feet, ta...\n",
       "4    [aded, basket, hte, mintue, person, store, pic...\n",
       "Name: tokens, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load stopwords from the provided file (stopwords_en.txt) into a lowercase set\n",
    "stopwords_set = set()\n",
    "with open(\"stopwords_en.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        w = line.strip()\n",
    "        if w:\n",
    "            stopwords_set.add(w.lower())\n",
    "print(\"Stopwords loaded:\", len(stopwords_set))\n",
    "\n",
    " \n",
    "\n",
    "# Normalize and remove very short tokens\n",
    "# single-character tokens are mostly noise, lowercasing reduces case variants.\n",
    "def clean_tokens(tokens):\n",
    "    cleaned = []\n",
    "    for t in tokens:\n",
    "        t = t.lower()\n",
    "        if len(t) == 1:     # remove only single-character words\n",
    "            continue\n",
    "        if t in stopwords_set:  # now 'stopwords' is your set\n",
    "            continue\n",
    "        cleaned.append(t)\n",
    "    return cleaned\n",
    "\n",
    "# Apply\n",
    "df[\"tokens\"] = df[\"tokens\"].apply(clean_tokens)\n",
    "df[\"tokens\"].head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [high, hopes, dress, wanted, work, initially, ...\n",
       "1    [love, love, love, jumpsuit, fun, flirty, fabu...\n",
       "2    [shirt, flattering, due, adjustable, front, ti...\n",
       "3    [love, tracy, reese, dresses, petite, feet, ta...\n",
       "4    [aded, basket, hte, mintue, person, store, pic...\n",
       "Name: tokens, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Remove stopwords — keeps content words for downstream modelling \n",
    "def remove_stopwords(tokens):\n",
    "    return [t for t in tokens if t not in stopwords_set]\n",
    "\n",
    "df[\"tokens\"] = df[\"tokens\"].apply(remove_stopwords)\n",
    "df[\"tokens\"].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size after removing singletons: 7549\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Rare-term pruning (TF=1): remove terms that appear only once in the entire corpus\n",
    "# This uses term frequency (total occurrences), not document frequency.\n",
    " \n",
    "all_tokens = [t for doc in df[\"tokens\"] for t in doc]\n",
    "freq = nltk.FreqDist(all_tokens)\n",
    "\n",
    "# Words appearing only once\n",
    "singleton_words = {w for w, c in freq.items() if c == 1}\n",
    "\n",
    "def remove_singletons(tokens):\n",
    "    return [t for t in tokens if t not in singleton_words]\n",
    "\n",
    "\n",
    "df[\"tokens\"] = df[\"tokens\"].apply(remove_singletons)\n",
    "print(\"Vocabulary size after removing singletons:\", len(set([t for doc in df[\"tokens\"] for t in doc])))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-20 removed: ['love', 'size', 'fit', 'dress', 'wear', 'top', 'great', 'fabric', 'color', 'small', 'ordered', 'perfect', 'flattering', 'soft', 'comfortable', 'back', 'cute', 'fits', 'nice', 'bought']\n"
     ]
    }
   ],
   "source": [
    " \n",
    "# High-DF pruning: remove the 20 most frequent words by Document Frequency (DF)\n",
    "# DF counts in how many documents a word appears, use set(doc) to avoid double-counting.\n",
    "\n",
    "df_counter = Counter()\n",
    "for doc in df[\"tokens\"]:\n",
    "    df_counter.update(set(doc))\n",
    "\n",
    "top20 = [w for w, _ in df_counter.most_common(20)]\n",
    "top20_set = set(top20)\n",
    "\n",
    "def remove_top20(tokens):\n",
    "    return [t for t in tokens if t not in top20_set]\n",
    "\n",
    "df[\"tokens\"] = df[\"tokens\"].apply(remove_top20)\n",
    " \n",
    "print(\"Top-20 removed:\", top20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving required outputs\n",
    "export the cleaned tokens (space-joined) to processed.csv, and write an alphabetically sorted unigram vocabulary with indices starting at 0 to vocab.txt for consistent word–index mapping in later tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved processed.csv\n"
     ]
    }
   ],
   "source": [
    "# code to save output data...\n",
    " \n",
    "# Save processed.csv — one review per line, tokens space-joined\n",
    " \n",
    "processed_texts = df[\"tokens\"].apply(lambda x: \" \".join(x))\n",
    "processed_texts.to_csv(\"processed.csv\", index=False,header='Review Text')\n",
    "print(\"Saved processed.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 7529\n",
      "Saved vocab.txt\n"
     ]
    }
   ],
   "source": [
    " \n",
    "# Build vocab.txt — unique tokens, alphabetically sorted, with indices starting at 0 \n",
    " \n",
    "vocab = sorted(set([t for doc in df[\"tokens\"] for t in doc]))\n",
    "with open(\"vocab.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for idx, w in enumerate(vocab):\n",
    "        f.write(f\"{w}:{idx}\\n\")\n",
    "\n",
    "print(\"Vocabulary size:\", len(vocab))\n",
    "print(\"Saved vocab.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 processed reviews:\n",
      "0    high hopes wanted work initially petite usual ...\n",
      "1        jumpsuit fun flirty fabulous time compliments\n",
      "2    shirt due adjustable front tie length leggings...\n",
      "3    tracy reese dresses petite feet tall brand pre...\n",
      "4    basket hte person store pick teh pale hte gorg...\n",
      "Name: tokens, dtype: object\n",
      "\n",
      "First 10 vocab entries:\n",
      "['a-cup', 'a-flutter', 'a-frame', 'a-kind', 'a-line', 'a-lines', 'a-symmetric', 'aa', 'ab', 'abbey']\n"
     ]
    }
   ],
   "source": [
    " \n",
    "# Quick Sanity Check\n",
    "\n",
    "print(\"First 5 processed reviews:\")\n",
    "print(processed_texts.head())\n",
    "\n",
    "print(\"\\nFirst 10 vocab entries:\")\n",
    "print(vocab[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    " \n",
    "In Task 1, transformed the raw clothing reviews into a clean, model-ready format. Using the required tokeniser, lower-cased text, removed very short tokens, filtered stopwords, and pruned both rare (TF=1) and overly common (top-20 DF) terms. The outputs are **processed.csv** (cleaned tokens per review) and **vocab.txt** (alphabetically sorted **word:index** mapping starting at 0). This pipeline reduces noise and sparsity, preserves meaningful words, and provides a reproducible foundation for Task 2 feature construction and Task 3 classification.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
