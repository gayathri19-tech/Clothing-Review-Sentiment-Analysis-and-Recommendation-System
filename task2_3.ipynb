{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: Milestone I Natural Language Processing\n",
    "## Task 2&3\n",
    "#### Student Name: Gayathri Devi Thotappa\n",
    "#### Student ID: s4111690\n",
    "\n",
    "\n",
    "Environment: Python 3 and Jupyter notebook\n",
    "\n",
    "Libraries used:\n",
    "* pandas\n",
    "* re\n",
    "* numpy\n",
    "* sklearn\n",
    "* tqdm\n",
    "* gensim \n",
    "\n",
    "## Introduction\n",
    "This notebook completes Task 2 (features) and Task 3 (classification) for the clothing-reviews dataset (~19.6k items). From the cleaned Review Text (Task 1), we build:\n",
    "\n",
    "Task 2:\n",
    "\n",
    "* Bag-of-Words (Count vectors) using the Task-1 vocabulary → count_vectors.txt.\n",
    "\n",
    "* Embeddings: one model (FastText) as unweighted and TF-IDF-weighted document vectors.\n",
    "\n",
    "Task 3:\n",
    "\n",
    "Q1: Compare BoW vs. unweighted vs. TF-IDF-weighted embeddings with the same classifier.\n",
    "\n",
    "Q2: Test information gain using BoW on Title only, Text only, and Title + Text.\n",
    "\n",
    "Protocol: 5-fold stratified CV with Logistic Regression, reporting Accuracy, Precision (macro), Recall (macro), F1 (macro) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to import libraries as you need in this assessment, e.g.,\n",
    "import os, re, ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2. Generating Feature Representations for Clothing Items Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the cleaned Review Text from Task 1 into three document representations (Title ignored as specified):\n",
    "\n",
    "Bag-of-Words (Count vectors): built strictly from vocab.txt; saved in sparse format as count_vectors.txt \n",
    "\n",
    "Embeddings — Unweighted: average of word vectors from the chosen model (FastText); saved as fasttext_unweighted.csv.\n",
    "\n",
    "Embeddings — TF-IDF Weighted: weighted average using TF-IDF to emphasise informative words; saved as fasttext_tfidf.csv.\n",
    "These representations balance interpretability (BoW) and semantic signal (embeddings), and will be used unchanged in Task 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2 uses only the cleaned Review Text from Task 1 (Title is ignored here as per spec).\n",
    "#  lock CountVectorizer's vocabulary to it for consistent indices from vocab.txt \n",
    "PROCESSED_CSV_PATH = \"processed.csv\"             # from Task-1\n",
    "VOCAB_TXT_PATH     = \"vocab.txt\"                 # from Task-1  \n",
    "\n",
    "COUNT_VECTORS_TXT  = \"count_vectors.txt\"         # required output format\n",
    "\n",
    "# Download, unzip, and point to the .vec file:\n",
    "#Link to download fasttext vec pretrained model: https://fasttext.cc/docs/en/english-vectors.html \n",
    "FASTTEXT_VEC_PATH  = \"wiki-news-300d-1M-subword.vec\"\n",
    "\n",
    "FT_UNW_NPY         = \"fasttext_unweighted.npy\"\n",
    "FT_UNW_CSV         = \"fasttext_unweighted.csv\"\n",
    "FT_TFIDF_NPY       = \"fasttext_tfidf.npy\"\n",
    "FT_TFIDF_CSV       = \"fasttext_tfidf.csv\"\n",
    "\n",
    "# processed.csv contains tokenized text so set TOKEN_COL:\n",
    "TOKEN_COL  = 'tokens'            \n",
    "REVIEW_COL = \"Review Text\"   # used if TOKEN_COL is None\n",
    "\n",
    "# regex from Task-1 spec\n",
    "TOKEN_REGEX = r\"[a-zA-Z]+(?:[-'][a-zA-Z]+)?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenization: regex + lowercasing.\n",
    "def tokenize_with_regex(text: str):\n",
    "    if not isinstance(text, str): return []\n",
    "    return [t.lower() for t in re.findall(TOKEN_REGEX, text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If tokens are stored as list-like strings, parse safely. Else fall back to space split.\n",
    "def parse_tokens_cell(x):\n",
    "    if isinstance(x, list):\n",
    "        return [str(t).lower() for t in x]\n",
    "    if isinstance(x, str):\n",
    "        s = x.strip()\n",
    "        if (s.startswith(\"[\") and s.endswith(\"]\")) or (s.startswith(\"(\") and s.endswith(\")\")):\n",
    "            try:\n",
    "                return [str(t).lower() for t in ast.literal_eval(s)]\n",
    "            except Exception:\n",
    "                return [tok.lower() for tok in s.split()]\n",
    "        return [tok.lower() for tok in s.split()]\n",
    "    return [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 19662 documents from processed.csv\n"
     ]
    }
   ],
   "source": [
    "# Load processed data\n",
    "assert os.path.exists(PROCESSED_CSV_PATH), \"processed.csv not found.\"\n",
    "df = pd.read_csv(PROCESSED_CSV_PATH)\n",
    "\n",
    "# Decide how to read tokens  \n",
    "if TOKEN_COL and TOKEN_COL in df.columns:\n",
    "    docs_tokens = df[TOKEN_COL].apply(parse_tokens_cell).tolist()\n",
    "else:\n",
    "    assert REVIEW_COL in df.columns, f\"'{REVIEW_COL}' not found in processed.csv\"\n",
    "    docs_tokens = df[REVIEW_COL].fillna(\"\").apply(tokenize_with_regex).tolist()\n",
    "\n",
    "n_docs = len(docs_tokens)\n",
    "print(f\"Loaded {n_docs} documents from processed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded vocab size = 7529\n"
     ]
    }
   ],
   "source": [
    "# Load Task-1 vocabulary\n",
    "# Expected format per line: \"word:idx\"\n",
    "assert os.path.exists(VOCAB_TXT_PATH), \"vocab.txt not found.\"\n",
    "v2i = {}\n",
    "with open(VOCAB_TXT_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        s = line.strip()\n",
    "        if not s: continue\n",
    "        w, idx = s.split(\":\")\n",
    "        v2i[w] = int(idx)\n",
    "\n",
    "V = len(v2i)\n",
    "i2v = {i:w for w,i in v2i.items()}\n",
    "assert set(v2i.values()) == set(range(V)), \"Vocab indices must be contiguous 0..V-1\"\n",
    "print(f\"Loaded vocab size = {V}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write sparse Count vectors\n",
    "def doc_sparse_counts(tokens):\n",
    "    c = Counter(t for t in tokens if t in v2i)         # restrict to Task-1 vocab\n",
    "    return sorted(((v2i[w], f) for w, f in c.items()), key=lambda x: x[0])  # index-ascending\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built TF-IDF matrix with fixed vocabulary.\n"
     ]
    }
   ],
   "source": [
    "# Build TF-IDF \n",
    "# pre-tokenised → pass space-joined text, provide vocabulary mapping\n",
    "docs_space = [\" \".join([t for t in toks if t in v2i]) for toks in docs_tokens]\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "    vocabulary=v2i,        # FIXED mapping (word -> column index)\n",
    "    lowercase=False,\n",
    "    tokenizer=str.split,   # tokens are space-separated already\n",
    "    preprocessor=None,\n",
    "    token_pattern=None     # don't re-tokenise; use our tokens\n",
    ")\n",
    "tfidf_mat = tfidf.fit_transform(docs_space).tocsr()   # shape: (n_docs, V)\n",
    "print(\"Built TF-IDF matrix with fixed vocabulary.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded FastText vectors (wiki-news-300d-1M-subword.vec), dim = 300\n"
     ]
    }
   ],
   "source": [
    "# Embedding features:\n",
    "# - Unweighted doc vector: mean of token embeddings -> fasttext_unweighted.csv\n",
    "# - TF-IDF weighted doc vector: weighted mean -> fasttext_tfidf.csv\n",
    "# Rows align with processed.csv (one vector per review).\n",
    "assert os.path.exists(FASTTEXT_VEC_PATH), (\n",
    "    \"FastText .vec not found. Unzip wiki-news-300d-1M-subword.vec.zip and set FASTTEXT_VEC_PATH.\" \n",
    ")\n",
    "ft = KeyedVectors.load_word2vec_format(FASTTEXT_VEC_PATH, binary=False)\n",
    "ft_dim = ft.vector_size\n",
    "print(f\"Loaded FastText vectors ({FASTTEXT_VEC_PATH}), dim = {ft_dim}\")\n",
    "\n",
    "#Return FastText vector if available; else None\n",
    "def get_vec(tok):\n",
    "    return ft[tok] if tok in ft else None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unweighted embeddings: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 19662/19662 [00:01<00:00, 10827.99it/s]\n",
      "TF-IDF weighted embeddings: 100%|██████████████████████████████████████████████████████████████████████████████████████████████| 19662/19662 [00:03<00:00, 4962.62it/s]\n"
     ]
    }
   ],
   "source": [
    "# Document Embeddings \n",
    "# (a) Unweighted mean of word vectors (bag-of-embeddings)\n",
    "# (b) TF-IDF weighted mean of word vectors (weighted bag-of-embeddings)\n",
    "\n",
    "emb_unw   = np.zeros((n_docs, ft_dim), dtype=\"float32\")\n",
    "emb_tfidf = np.zeros((n_docs, ft_dim), dtype=\"float32\")\n",
    "\n",
    "# Unweighted mean\n",
    "for i, toks in tqdm(list(enumerate(docs_tokens)), desc=\"Unweighted embeddings\"):\n",
    "    toks_v = [t for t in toks if t in v2i]   # stay consistent with vocab\n",
    "    if not toks_v: \n",
    "        continue\n",
    "    vecs = [get_vec(t) for t in toks_v]\n",
    "    vecs = [v for v in vecs if v is not None]\n",
    "    if vecs:\n",
    "        emb_unw[i] = np.mean(vecs, axis=0)\n",
    "\n",
    "# TF-IDF weighted mean\n",
    "vocab_inv = i2v\n",
    "for i in tqdm(range(n_docs), desc=\"TF-IDF weighted embeddings\"):\n",
    "    row = tfidf_mat.getrow(i)\n",
    "    if row.nnz == 0: \n",
    "        continue\n",
    "    idxs, vals = row.indices, row.data\n",
    "    num = np.zeros(ft_dim, dtype=\"float32\")\n",
    "    den = 0.0\n",
    "    for j, idx in enumerate(idxs):\n",
    "        term = vocab_inv[idx]\n",
    "        v = get_vec(term)\n",
    "        if v is None: \n",
    "            continue\n",
    "        w = float(vals[j])\n",
    "        num += w * v\n",
    "        den += w\n",
    "    if den > 0:\n",
    "        emb_tfidf[i] = num / den"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving outputs\n",
    "Save the BoW count vectors to count_vectors.txt in the required sparse format—one review per line as #row_id, word_index:frequency, built from the Task-1 vocabulary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote count_vectors.txt\n",
      "\n",
      "Artifacts saved:\n",
      "  - count_vectors.txt\n",
      "  - fasttext_unweighted.npy, fasttext_unweighted.csv\n",
      "  - fasttext_tfidf.npy, fasttext_tfidf.csv\n"
     ]
    }
   ],
   "source": [
    "# code to save output data...\n",
    "with open(COUNT_VECTORS_TXT, \"w\", encoding=\"utf-8\") as out:\n",
    "    for doc_id, toks in enumerate(docs_tokens):\n",
    "        pairs = doc_sparse_counts(toks)\n",
    "        out.write(f\"#{doc_id},\")\n",
    "        if pairs:\n",
    "            out.write(\",\".join(f\"{i}:{f}\" for i,f in pairs))\n",
    "        out.write(\"\\n\")\n",
    "print(f\"Wrote {COUNT_VECTORS_TXT}\") \n",
    "\n",
    "np.save(FT_UNW_NPY,   emb_unw)\n",
    "pd.DataFrame(emb_unw).to_csv(FT_UNW_CSV, index=False)\n",
    "\n",
    "np.save(FT_TFIDF_NPY, emb_tfidf)\n",
    "pd.DataFrame(emb_tfidf).to_csv(FT_TFIDF_CSV, index=False)\n",
    "\n",
    "print(\"\\nArtifacts saved:\")\n",
    "print(f\"  - {COUNT_VECTORS_TXT}\")\n",
    "print(f\"  - {FT_UNW_NPY}, {FT_UNW_CSV}\")\n",
    "print(f\"  - {FT_TFIDF_NPY}, {FT_TFIDF_CSV}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sanity check:\n",
      "First 3 lines of count_vectors.txt:\n",
      "#0,687:1,1028:1,1716:1,1792:1,2289:1,2481:1,2602:1,2892:2,3010:1,3087:1,3193:1,3258:1,3549:2,3552:1,3832:1,3934:1,4224:2,4234:1,4427:1,4639:2,5260:1,5668:1,6726:1,7092:1,7207:1,7406:1,7520:1,7522:1\n",
      "#1,1287:1,2284:1,2502:1,2667:1,3403:1,6739:1\n",
      "#2,86:1,925:1,1988:1,2646:1,3584:1,3595:1,4506:1,5736:2,5924:1,6716:1\n",
      "Unweighted emb[0] L2-norm: 0.27935776\n",
      "TF-IDF   emb[0] L2-norm: 0.28576574\n"
     ]
    }
   ],
   "source": [
    "# Quick sanity checks\n",
    "print(\"\\nSanity check:\")\n",
    "print(\"First 3 lines of count_vectors.txt:\")\n",
    "with open(COUNT_VECTORS_TXT, \"r\", encoding=\"utf-8\") as f:\n",
    "    for _ in range(3):\n",
    "        line = f.readline()\n",
    "        if not line: break\n",
    "        print(line.strip())\n",
    "\n",
    "if n_docs > 0:\n",
    "    print(\"Unweighted emb[0] L2-norm:\", np.linalg.norm(emb_unw[0]))\n",
    "    print(\"TF-IDF   emb[0] L2-norm:\", np.linalg.norm(emb_tfidf[0])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3. Clothing Review Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict Recommended IND (0/1) using Logistic Regression with 5-fold stratified cross-validation, reporting Accuracy, Precision (macro), Recall (macro), and F1 (macro):\n",
    "\n",
    "Q1: Language model comparison — Train/evaluate on each Task-2 feature:\n",
    "BoW (from count_vectors.txt), Embedding (Unweighted), and Embedding (TF-IDF Weighted), holding the classifier constant for a fair comparison.\n",
    "\n",
    "Q2: Does more information help? — Using BoW, compare Title only, Text only, and Title + Text (concatenated)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to perform the task...\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "from sklearn.model_selection import StratifiedKFold, cross_validate\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data (label, title, text)\n",
    "# Task 3 needs labels (Recommended IND) and Title from assignment3.csv,\n",
    "# plus cleaned Review Text tokens from processed.csv. We align row counts to the minimum to stay safe.\n",
    "raw = pd.read_csv(\"assignment3.csv\")       # Title, Recommended IND\n",
    "proc = pd.read_csv(\"processed.csv\")        # tokens (cleaned Review Text)\n",
    "\n",
    "# Defensive row alignment\n",
    "n = min(len(raw), len(proc))\n",
    "raw  = raw.iloc[:n].reset_index(drop=True)\n",
    "proc = proc.iloc[:n].reset_index(drop=True)\n",
    "\n",
    "# Label (0/1)\n",
    "y = raw[\"Recommended IND\"].astype(int).values\n",
    "\n",
    "# Text sources\n",
    "title = raw[\"Title\"].fillna(\"\").astype(str) if \"Title\" in raw.columns else pd.Series([\"\"]*n)\n",
    "text  = proc[\"tokens\"].fillna(\"\").astype(str)\n",
    "\n",
    "# Exact token regex required in the brief\n",
    "TOKEN_PATTERN = r\"[a-zA-Z]+(?:[-'][a-zA-Z]+)?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Cross validation, metrics, classifier\n",
    " \n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scoring = {\n",
    "    \"accuracy\": make_scorer(accuracy_score),\n",
    "    \"precision_macro\": make_scorer(precision_score, average=\"macro\", zero_division=0),\n",
    "    \"recall_macro\": make_scorer(recall_score, average=\"macro\", zero_division=0),\n",
    "    \"f1_macro\": make_scorer(f1_score, average=\"macro\", zero_division=0),\n",
    "}\n",
    "clf = LogisticRegression(max_iter=2000, n_jobs=None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BoW with required token pattern; 5-fold CV\n",
    "def eval_bow(name, series, y_vec):\n",
    "    X = CountVectorizer(token_pattern=TOKEN_PATTERN, lowercase=True).fit_transform(series)\n",
    "    res = cross_validate(clf, X, y_vec, cv=cv, scoring=scoring, return_train_score=False)\n",
    "    return dict(\n",
    "        experiment=name,\n",
    "        accuracy_mean=np.mean(res[\"test_accuracy\"]),\n",
    "        accuracy_std=np.std(res[\"test_accuracy\"]),\n",
    "        precision_macro_mean=np.mean(res[\"test_precision_macro\"]),\n",
    "        recall_macro_mean=np.mean(res[\"test_recall_macro\"]),\n",
    "        f1_macro_mean=np.mean(res[\"test_f1_macro\"]),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load optional embedding matrix (rows=docs, cols=dims). Returns np.ndarray or None.\n",
    "def try_load_embeddings(path):\n",
    "    if not os.path.exists(path):\n",
    "        return None\n",
    "    arr = pd.read_csv(path, header=None)\n",
    "    for c in arr.columns:\n",
    "        arr[c] = pd.to_numeric(arr[c], errors=\"coerce\")\n",
    "    return arr.fillna(0.0).values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate a ready numeric/sparse matrix with 5-fold CV\n",
    "def eval_dense(name, X, y_vec):\n",
    "    res = cross_validate(clf, X, y_vec, cv=cv, scoring=scoring, return_train_score=False)\n",
    "    return dict(\n",
    "        experiment=name,\n",
    "        accuracy_mean=np.mean(res[\"test_accuracy\"]),\n",
    "        accuracy_std=np.std(res[\"test_accuracy\"]),\n",
    "        precision_macro_mean=np.mean(res[\"test_precision_macro\"]),\n",
    "        recall_macro_mean=np.mean(res[\"test_recall_macro\"]),\n",
    "        f1_macro_mean=np.mean(res[\"test_f1_macro\"]),\n",
    "    ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reads Task-2 count_vectors.txt with lines like:\n",
    "#<row_id>, widx:freq,widx:freq,...\n",
    "#Returns csr_matrix (n_docs x vocab_size_in_file)\n",
    "    \n",
    "def load_count_vectors(path):\n",
    "    if not os.path.exists(path): \n",
    "        return None\n",
    "    rows, cols, data = [], [], []\n",
    "    max_col, row_idx = -1, 0\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            s = line.strip()\n",
    "            if not s:\n",
    "                row_idx += 1\n",
    "                continue\n",
    "            # Strip \"#<row>,\" if present\n",
    "            if s.startswith(\"#\"):\n",
    "                k = s.find(\",\")\n",
    "                s = s[k+1:] if k != -1 else \"\"\n",
    "            if s:\n",
    "                for part in s.split(\",\"):\n",
    "                    part = part.strip()\n",
    "                    if \":\" not in part: \n",
    "                        continue\n",
    "                    widx, freq = part.split(\":\", 1)\n",
    "                    try:\n",
    "                        c = int(widx); v = float(freq)\n",
    "                    except:\n",
    "                        continue\n",
    "                    rows.append(row_idx); cols.append(c); data.append(v)\n",
    "                    if c > max_col: max_col = c\n",
    "            row_idx += 1\n",
    "    n_docs = row_idx\n",
    "    n_cols = (max_col + 1) if max_col >= 0 else 0\n",
    "    return sparse.csr_matrix((data, (rows, cols)), shape=(n_docs, n_cols), dtype=np.float64)\n",
    "\n",
    "results = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Q1 — Language model comparisons (same classifier)\n",
    "#     A) BoW (Review Text)  -> prefer count_vectors.txt\n",
    "#     B) FastText (Unweighted)       [if file exists]\n",
    "#     C) FastText (TF-IDF weighted)  [if file exists]\n",
    "# -----------------------------\n",
    "# A) BoW via count_vectors.txt  \n",
    "X_bow = load_count_vectors(\"count_vectors.txt\")\n",
    "if X_bow is not None:\n",
    "    # Align rows if needed\n",
    "    m = min(X_bow.shape[0], len(y))\n",
    "    X_bow = X_bow[:m]\n",
    "    y_q1  = y[:m]\n",
    "    results.append(eval_dense(\"Q1: BoW (from count_vectors.txt)\", X_bow, y_q1))\n",
    "else:\n",
    "    # Fallback: build BoW directly from cleaned text using the required regex\n",
    "    results.append(eval_bow(\"Q1: BoW (Review Text via CountVectorizer)\", text, y))\n",
    "\n",
    "# B) FastText (Unweighted)\n",
    "X_unw = try_load_embeddings(\"fasttext_unweighted.csv\")\n",
    "if X_unw is not None:\n",
    "    m = min(len(y), X_unw.shape[0])\n",
    "    results.append(eval_dense(\"Q1: FastText (Unweighted avg)\", X_unw[:m], y[:m]))\n",
    "    # keep y/text/title aligned with m for subsequent experiments\n",
    "    y = y[:m]; text = text.iloc[:m]; title = title.iloc[:m]\n",
    "\n",
    "# C) FastText (TF-IDF weighted)\n",
    "X_w = try_load_embeddings(\"fasttext_tfidf.csv\")\n",
    "if X_w is not None:\n",
    "    m = min(len(y), X_w.shape[0])\n",
    "    results.append(eval_dense(\"Q1: FastText (TF-IDF weighted avg)\", X_w[:m], y[:m]))\n",
    "    y = y[:m]; text = text.iloc[:m]; title = title.iloc[:m]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Q2 — Does more information help? (BoW)\n",
    "#     - Title only\n",
    "#     - Text only\n",
    "#     - Title + Text\n",
    "# -----------------------------\n",
    "results.append(eval_bow(\"Q2: BoW (Title only)\", title, y))\n",
    "results.append(eval_bow(\"Q2: BoW (Text only)\", text, y))\n",
    "results.append(eval_bow(\"Q2: BoW (Title + Text)\", (title + \" \" + text).str.strip(), y)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        experiment  accuracy_mean  accuracy_std  precision_macro_mean  recall_macro_mean  f1_macro_mean\n",
      "  Q1: BoW (from count_vectors.txt)       0.876005      0.002158              0.804900           0.747241       0.770586\n",
      "Q1: FastText (TF-IDF weighted avg)       0.818228      0.000095              0.509109           0.500140       0.450291\n",
      "     Q1: FastText (Unweighted avg)       0.818228      0.000095              0.509109           0.500140       0.450291\n",
      "               Q2: BoW (Text only)       0.876005      0.002158              0.804900           0.747241       0.770586\n",
      "            Q2: BoW (Title + Text)       0.898993      0.002335              0.840401           0.805780       0.821321\n",
      "              Q2: BoW (Title only)       0.884600      0.002986              0.823329           0.760652       0.785952\n"
     ]
    }
   ],
   "source": [
    "# Save & print\n",
    "res_df = pd.DataFrame(results).sort_values(\"experiment\")\n",
    "res_df.to_csv(\"task3_results.csv\", index=False)\n",
    "print(res_df.to_string(index=False)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "For Task 2, engineered three document representations from the cleaned Review Text: Bag-of-Words count vectors built from the Task-1 vocabulary and saved as count_vectors.txt, unweighted embedding document vectors (mean of word embeddings) saved as fasttext_unweighted.csv, and TF-IDF-weighted embedding vectors saved as fasttext_tfidf.csv. For Task 3, predicted Recommended IND (0/1) using Logistic Regression with 5-fold stratified cross-validation, reporting Accuracy, Precision (macro), Recall (macro), and F1 (macro). first compared the three Task-2 feature types (Q1), then assessed whether adding more information helps by training BoW models on Title only, Text only, and Title + Text (Q2). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
